{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 16 key-value pairs and 291 tensors from /Users/alanngo/Desktop/projects/models/llama2/ggml-model-q5_k_m.gguf.bin (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = llama2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 17\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q5_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 4.45 GiB (5.68 BPW) \n",
      "llm_load_print_meta: general.name     = llama2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
      "ggml_backend_metal_buffer_from_ptr: allocated buffer, size =  4474.94 MiB, ( 9281.61 / 21845.34)\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =    85.94 MiB\n",
      "llm_load_tensors:      Metal buffer size =  4474.93 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: GGML_METAL_PATH_RESOURCES = nil\n",
      "ggml_metal_init: loading '/Users/alanngo/Library/Python/3.9/lib/python/site-packages/llama_cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 22906.50 MB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   256.00 MiB, ( 9537.61 / 21845.34)\n",
      "llama_kv_cache_init:      Metal KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =     0.02 MiB, ( 9537.62 / 21845.34)\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =    73.02 MiB, ( 9610.62 / 21845.34)\n",
      "llama_new_context_with_model: graph splits (measure): 3\n",
      "llama_new_context_with_model:      Metal compute buffer size =    73.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =     9.00 MiB\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '32', 'llama.context_length': '2048', 'llama.attention.head_count': '32', 'llama.rope.dimension_count': '128', 'general.file_type': '17', 'llama.feed_forward_length': '11008', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'general.name': 'llama2'}\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# maybe not fit with Llama??????\n",
    "# prompt = ChatPromptTemplate.from_messages([\n",
    "#     (\"system\", \"\"\"you are a sommelier and only answer alcohol related questions. \n",
    "#      Your response are very concise.\"\"\"),\n",
    "#     (\"user\", \"{input}\")\n",
    "# ])\n",
    "\n",
    "# template = \"\"\"\n",
    "# For red wine suggestion, provide the following information:\n",
    "# grapes: the suggested wines contain these kinds of grapes\n",
    "# Format the output only as JSON with the following keys and value being an array:\n",
    "# grapes:[]\n",
    "\n",
    "# [INST] {input} [/INST]\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "template = \"\"\"\n",
    "For red wine suggestion, provide the following information:\n",
    "Give the best grape you think is most appropriate for the food. \n",
    "Select your answer from the following list: [\"Merlot\", \"Malbec\", \"Tannat\"]\n",
    "Limit your answer to one word.\n",
    "\n",
    "[INST] {input} [/INST]\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"input\"])\n",
    "\n",
    "# Callbacks support token-wise streaming\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "n_gpu_layers = 40  # Change this value based on your model and your GPU VRAM pool.\n",
    "n_batch = 512  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "\n",
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"/Users/alanngo/Desktop/projects/models/llama2/ggml-model-q5_k_m.gguf.bin\",\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True  # Verbose is required to pass to the callback manager\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm, verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nyou are a sommelier and only answer alcohol related questions. \\nYour response are very concise.\\n\\nQuestion: What wine best with fish\\nAnswer:\\n'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.format(input=\"What wine best with fish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_list = [\"What wine best with fish\",\n",
    "                 \"What wine best with steak\",\n",
    "                 \"what wine best with human\",\n",
    "                 \"I am having a chinese food tonight\",\n",
    "                 \"I am having a japanese food tonight and I have limited budget\",]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For red wine pairing suggestions, I would recommend a Tannat wine. Tannat is a full-bodied grape variety that is well-suited for rich and savory dishes, such as beef or lamb. Its tannins can help to balance the fattiness of the meat, while its fruit flavors can complement the bold seasonings used in the dish.\n",
      "Here are some specific red wine pairing suggestions that would go well with your beef Wellington:\n",
      "* Tannat (full-bodied and structured)\n",
      "Alternatively, you could also consider a Malbec or Merlot wine. Both of these varieties have a soft, fruity taste that can complement the rich flavors of the beef without overpowering it. However, if you prefer a more full-bodied wine with more tannins, Tannat would be the best choice.For a hearty be"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     519.94 ms\n",
      "llama_print_timings:      sample time =      28.97 ms /   196 runs   (    0.15 ms per token,  6764.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =     519.86 ms /    63 tokens (    8.25 ms per token,   121.19 tokens per second)\n",
      "llama_print_timings:        eval time =    6845.89 ms /   195 runs   (   35.11 ms per token,    28.48 tokens per second)\n",
      "llama_print_timings:       total time =    7958.20 ms /   258 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ef stew, I would recommend a full-bodied red wine that can stand up to the rich flavors of the dish. Of the options listed, I think Merlot would be the best choice. Merlot is known for its smooth, velvety texture and its ability to blend well with a variety of flavors, making it an excellent pairing for the bold flavors of the stew.For red wine, I"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     519.94 ms\n",
      "llama_print_timings:      sample time =      16.92 ms /    92 runs   (    0.18 ms per token,  5438.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    3229.59 ms /    92 runs   (   35.10 ms per token,    28.49 tokens per second)\n",
      "llama_print_timings:       total time =    3556.60 ms /    93 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " would suggest a Cabernet Sauvignon. This full-bodied wine pairs well with the rich and savory flavors of the beef Wellington. The tannins in the Cabernet Sauvignon will complement the meat's juiciness and the oak notes will enhance the dish's complexity.Based on the d"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     519.94 ms\n",
      "llama_print_timings:      sample time =      11.18 ms /    76 runs   (    0.15 ms per token,  6797.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    2644.86 ms /    76 runs   (   34.80 ms per token,    28.73 tokens per second)\n",
      "llama_print_timings:       total time =    2853.83 ms /    77 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ish you provided, I would recommend a red wine with a rich and full-bodied character to complement the bold flavors of the beef. Of the options listed, I think Merlot would be the best choice. Merlot is known for its smooth tannins and velvety texture, which will help to balance out the richness of the beef without overpowering it. The fruit flavors in a good Merlot should also complement the sweetness of the sauce and add depth to the overall dish.\n",
      "Therefore, my answer is: Merlot.For red wine pairing"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     519.94 ms\n",
      "llama_print_timings:      sample time =      20.40 ms /   126 runs   (    0.16 ms per token,  6177.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    4406.23 ms /   126 runs   (   34.97 ms per token,    28.60 tokens per second)\n",
      "llama_print_timings:       total time =    4805.59 ms /   127 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " suggestions, I would recommend a Merlot for the beef dish. Merlot is a versatile grape that pairs well with a variety of foods, including red meat. Its smooth tannins and fruity flavors complement the richness of the beef without overpowering it."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     519.94 ms\n",
      "llama_print_timings:      sample time =      11.90 ms /    68 runs   (    0.18 ms per token,  5711.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    2373.92 ms /    68 runs   (   34.91 ms per token,    28.64 tokens per second)\n",
      "llama_print_timings:       total time =    2606.01 ms /    69 tokens\n"
     ]
    }
   ],
   "source": [
    "for question in question_list:\n",
    "    prompt.format(input=question)\n",
    "    llm_chain = LLMChain(prompt=prompt, llm=llm, verbose=False)\n",
    "\n",
    "    llm_chain.run(\"?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
